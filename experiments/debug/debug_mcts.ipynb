{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug trained MCTS agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from ginkgo_rl import GinkgoLikelihood1DEnv, MCTSAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.DEBUG\n",
    ")\n",
    "\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"ginkgo_rl\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing environment\n",
      "Creating linear layer: 100->100\n",
      "Creating linear head layer: 100->1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GinkgoLikelihood1DEnv()\n",
    "agent = MCTSAgent(env, verbose=1)\n",
    "agent.load_state_dict(torch.load(\"../data/runs/mcts_20200901_174303/model.pty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resetting environment\n",
      "Sampling new jet with 9 leaves\n",
      "9 particles:\n",
      "  p[ 0] = (  1.3,   0.9,   0.7,   0.7)\n",
      "  p[ 1] = (  0.8,   0.3,   0.5,   0.5)\n",
      "  p[ 2] = (  0.6,   0.3,   0.3,   0.3)\n",
      "  p[ 3] = (  0.5,   0.4,   0.3,   0.2)\n",
      "  p[ 4] = (  0.3,   0.2,   0.2,   0.2)\n",
      "  p[ 5] = (  0.2,   0.1,   0.1,   0.1)\n",
      "  p[ 6] = (  0.2,   0.1,   0.1,   0.1)\n",
      "  p[ 7] = (  0.1,   0.0,   0.1,   0.1)\n",
      "  p[ 8] = (  0.0,   0.0,   0.0,   0.0)\n",
      "Starting MCTS with 100 trajectories\n",
      "MCTS results:\n",
      "     0: log likelihood =  -8.4, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "     1: log likelihood =  -5.3, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "     2: log likelihood =  -4.6, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "     3: log likelihood =  -8.5, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "     4: log likelihood =  -9.7, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "     5: log likelihood =  -8.1, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "     6: log likelihood =  -6.7, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "     7: log likelihood = -100.0, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "     8: log likelihood =  -3.0, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "     9: log likelihood =  -8.0, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    10: log likelihood =  -4.3, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    11: log likelihood =  -3.8, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    12: log likelihood = -100.0, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    13: log likelihood =  -6.1, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "  g 14: log likelihood =  -2.6, policy = 0.01, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    15: log likelihood =  -6.1, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    16: log likelihood =  -3.8, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    17: log likelihood =  -3.7, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    18: log likelihood =  -6.9, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    19: log likelihood =  -3.0, policy = 0.01, n =  1, mean = -75.9 [0.94], max = -75.9 [0.94]\n",
      "    20: log likelihood =  -2.7, policy = 0.03, n =  3, mean = -76.1 [0.94], max = -76.0 [0.94]\n",
      "    21: log likelihood =  -8.4, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    22: log likelihood =  -6.0, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    23: log likelihood =  -6.1, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    24: log likelihood =  -8.1, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    25: log likelihood =  -4.8, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    26: log likelihood =  -4.2, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    27: log likelihood =  -4.2, policy = 0.01, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    28: log likelihood =  -5.5, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    29: log likelihood =  -5.1, policy = 0.00, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      "    30: log likelihood =  -4.2, policy = 0.01, n =  0, mean =   0.0 [0.93], max =  -inf [0.00]\n",
      " *  31: log likelihood =  -3.7, policy = 0.04, n = 11, mean = -80.9 [0.89], max = -70.0 [1.00]\n",
      "    32: log likelihood =  -3.8, policy = 0.04, n =  4, mean = -74.9 [0.95], max = -73.6 [0.97]\n",
      "    33: log likelihood =  -2.7, policy = 0.52, n = 48, mean = -77.0 [0.93], max = -70.2 [1.00]\n",
      "    34: log likelihood =  -3.2, policy = 0.20, n = 20, mean = -75.7 [0.95], max = -72.8 [0.97]\n",
      "    35: log likelihood =  -3.5, policy = 0.13, n = 13, mean = -76.0 [0.94], max = -74.6 [0.96]\n",
      "Environment step. Action: (8, 3)\n",
      "Computing log likelihood of action (8, 3): ti = 0.0, tj = 0.0, t_cut = 16.0, lam = 1.5 -> log likelihood = -3.7217040061950684\n",
      "Merging particles 8 and 3. New state has 8 particles.\n",
      "8 particles:\n",
      "  p[ 0] = (  1.3,   0.9,   0.7,   0.7)\n",
      "  p[ 1] = (  0.8,   0.3,   0.5,   0.5)\n",
      "  p[ 2] = (  0.6,   0.5,   0.3,   0.2)\n",
      "  p[ 3] = (  0.6,   0.3,   0.3,   0.3)\n",
      "  p[ 4] = (  0.3,   0.2,   0.2,   0.2)\n",
      "  p[ 5] = (  0.2,   0.1,   0.1,   0.1)\n",
      "  p[ 6] = (  0.2,   0.1,   0.1,   0.1)\n",
      "  p[ 7] = (  0.1,   0.0,   0.1,   0.1)\n",
      "Starting MCTS with 100 trajectories\n",
      "MCTS results:\n",
      "     0: log likelihood =  -8.4, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "     1: log likelihood = -11.3, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "     2: log likelihood = -12.3, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "     3: log likelihood =  -5.3, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "     4: log likelihood =  -4.6, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "     5: log likelihood = -10.7, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "     6: log likelihood =  -6.7, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "     7: log likelihood = -100.0, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "     8: log likelihood = -10.6, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "     9: log likelihood =  -3.0, policy = 0.03, n =  4, mean = -71.7 [0.94], max = -70.8 [0.95]\n",
      "    10: log likelihood =  -4.3, policy = 0.01, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "    11: log likelihood =  -3.8, policy = 0.01, n =  1, mean = -72.4 [0.94], max = -72.4 [0.94]\n",
      "    12: log likelihood =  -8.7, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "    13: log likelihood = -100.0, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      " *g 14: log likelihood =  -2.6, policy = 0.13, n = 23, mean = -70.8 [0.95], max = -65.5 [1.00]\n",
      "    15: log likelihood =  -6.1, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "    16: log likelihood =  -3.8, policy = 0.02, n =  3, mean = -72.5 [0.93], max = -72.3 [0.94]\n",
      "    17: log likelihood =  -9.4, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "    18: log likelihood =  -3.7, policy = 0.04, n =  5, mean = -72.4 [0.94], max = -71.0 [0.95]\n",
      "    19: log likelihood =  -3.0, policy = 0.18, n = 14, mean = -78.4 [0.88], max = -66.5 [0.99]\n",
      "    20: log likelihood =  -2.7, policy = 0.40, n = 46, mean = -74.1 [0.92], max = -69.7 [0.96]\n",
      "    21: log likelihood =  -8.4, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "    22: log likelihood =  -6.0, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "    23: log likelihood = -10.6, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "    24: log likelihood =  -6.1, policy = 0.00, n =  0, mean =   0.0 [0.92], max =  -inf [0.00]\n",
      "    25: log likelihood =  -4.8, policy = 0.02, n =  2, mean = -72.9 [0.93], max = -72.9 [0.93]\n",
      "    26: log likelihood =  -4.2, policy = 0.07, n =  9, mean = -73.0 [0.93], max = -71.8 [0.94]\n",
      "    27: log likelihood =  -4.2, policy = 0.09, n =  4, mean = -96.4 [0.71], max = -71.3 [0.95]\n",
      "Environment step. Action: (5, 4)\n",
      "Computing log likelihood of action (5, 4): ti = 0.0, tj = 0.0, t_cut = 16.0, lam = 1.5 -> log likelihood = -2.6181464195251465\n",
      "Merging particles 5 and 4. New state has 7 particles.\n",
      "7 particles:\n",
      "  p[ 0] = (  1.3,   0.9,   0.7,   0.7)\n",
      "  p[ 1] = (  0.8,   0.3,   0.5,   0.5)\n",
      "  p[ 2] = (  0.6,   0.5,   0.3,   0.2)\n",
      "  p[ 3] = (  0.6,   0.3,   0.3,   0.3)\n",
      "  p[ 4] = (  0.5,   0.3,   0.3,   0.3)\n",
      "  p[ 5] = (  0.2,   0.1,   0.1,   0.1)\n",
      "  p[ 6] = (  0.1,   0.0,   0.1,   0.1)\n",
      "Starting MCTS with 82 trajectories\n",
      "MCTS results:\n",
      "     0: log likelihood =  -8.4, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     1: log likelihood = -11.3, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     2: log likelihood = -12.3, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     3: log likelihood =  -5.3, policy = 0.01, n =  1, mean = -70.5 [0.94], max = -70.5 [0.94]\n",
      "     4: log likelihood =  -4.6, policy = 0.04, n =  4, mean = -67.8 [0.96], max = -66.3 [0.98]\n",
      "     5: log likelihood = -10.7, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     6: log likelihood =  -9.5, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     7: log likelihood =  -6.4, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     8: log likelihood = -14.1, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     9: log likelihood =  -6.2, policy = 0.01, n =  1, mean = -65.5 [0.98], max = -65.5 [0.98]\n",
      "    10: log likelihood =  -6.1, policy = 0.01, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      " *  11: log likelihood =  -3.8, policy = 0.20, n = 22, mean = -67.6 [0.96], max = -63.6 [1.00]\n",
      "    12: log likelihood =  -9.4, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "  g 13: log likelihood =  -3.7, policy = 0.32, n = 29, mean = -69.8 [0.94], max = -67.2 [0.97]\n",
      "    14: log likelihood =  -6.2, policy = 0.01, n =  4, mean = -71.8 [0.93], max = -71.1 [0.93]\n",
      "    15: log likelihood =  -8.4, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "    16: log likelihood =  -6.0, policy = 0.02, n =  1, mean = -69.6 [0.95], max = -69.6 [0.95]\n",
      "    17: log likelihood = -10.6, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "    18: log likelihood =  -6.1, policy = 0.02, n =  1, mean = -72.3 [0.92], max = -72.3 [0.92]\n",
      "    19: log likelihood =  -8.0, policy = 0.00, n =  5, mean = -72.4 [0.92], max = -69.8 [0.94]\n",
      "    20: log likelihood =  -4.2, policy = 0.36, n = 37, mean = -68.2 [0.96], max = -63.9 [1.00]\n",
      "Environment step. Action: (5, 1)\n",
      "Computing log likelihood of action (5, 1): ti = 0.0, tj = 0.0, t_cut = 16.0, lam = 1.5 -> log likelihood = -3.8469157218933105\n",
      "Merging particles 5 and 1. New state has 6 particles.\n",
      "6 particles:\n",
      "  p[ 0] = (  1.3,   0.9,   0.7,   0.7)\n",
      "  p[ 1] = (  0.9,   0.4,   0.6,   0.6)\n",
      "  p[ 2] = (  0.6,   0.5,   0.3,   0.2)\n",
      "  p[ 3] = (  0.6,   0.3,   0.3,   0.3)\n",
      "  p[ 4] = (  0.5,   0.3,   0.3,   0.3)\n",
      "  p[ 5] = (  0.1,   0.0,   0.1,   0.1)\n",
      "Starting MCTS with 53 trajectories\n",
      "MCTS results:\n",
      "     0: log likelihood = -11.3, policy = 0.00, n =  0, mean =   0.0 [0.96], max =  -inf [0.00]\n",
      "     1: log likelihood = -11.3, policy = 0.00, n =  0, mean =   0.0 [0.96], max =  -inf [0.00]\n",
      "     2: log likelihood = -15.6, policy = 0.00, n =  0, mean =   0.0 [0.96], max =  -inf [0.00]\n",
      " *g  3: log likelihood =  -5.3, policy = 0.39, n = 32, mean = -62.4 [0.98], max = -60.3 [1.00]\n",
      "     4: log likelihood =  -7.7, policy = 0.03, n =  1, mean = -64.6 [0.96], max = -64.6 [0.96]\n",
      "     5: log likelihood = -10.7, policy = 0.00, n =  0, mean =   0.0 [0.96], max =  -inf [0.00]\n",
      "     6: log likelihood =  -9.5, policy = 0.00, n =  0, mean =   0.0 [0.96], max =  -inf [0.00]\n",
      "     7: log likelihood = -10.9, policy = 0.00, n =  0, mean =   0.0 [0.96], max =  -inf [0.00]\n",
      "     8: log likelihood = -14.1, policy = 0.00, n =  0, mean =   0.0 [0.96], max =  -inf [0.00]\n",
      "     9: log likelihood =  -6.2, policy = 0.20, n = 15, mean = -62.8 [0.98], max = -61.7 [0.99]\n",
      "    10: log likelihood =  -8.4, policy = 0.02, n =  1, mean = -65.6 [0.95], max = -65.6 [0.95]\n",
      "    11: log likelihood =  -8.8, policy = 0.01, n =  9, mean = -64.6 [0.96], max = -63.6 [0.97]\n",
      "    12: log likelihood = -10.6, policy = 0.00, n =  0, mean =   0.0 [0.96], max =  -inf [0.00]\n",
      "    13: log likelihood =  -6.1, policy = 0.31, n = 15, mean = -69.2 [0.92], max = -65.2 [0.96]\n",
      "    14: log likelihood =  -8.0, policy = 0.03, n =  2, mean = -64.1 [0.97], max = -63.9 [0.97]\n",
      "Environment step. Action: (3, 0)\n",
      "Computing log likelihood of action (3, 0): ti = 0.0, tj = 0.0, t_cut = 16.0, lam = 1.5 -> log likelihood = -5.2660722732543945\n",
      "Merging particles 3 and 0. New state has 5 particles.\n",
      "5 particles:\n",
      "  p[ 0] = (  1.9,   1.2,   1.1,   1.1)\n",
      "  p[ 1] = (  0.9,   0.4,   0.6,   0.6)\n",
      "  p[ 2] = (  0.6,   0.5,   0.3,   0.2)\n",
      "  p[ 3] = (  0.5,   0.3,   0.3,   0.3)\n",
      "  p[ 4] = (  0.1,   0.0,   0.1,   0.1)\n",
      "Starting MCTS with 18 trajectories\n",
      "MCTS results:\n",
      "     0: log likelihood = -14.5, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     1: log likelihood = -15.3, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     2: log likelihood = -15.6, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     3: log likelihood = -12.6, policy = 0.00, n = 18, mean = -62.4 [0.93], max = -60.3 [0.95]\n",
      "     4: log likelihood = -10.9, policy = 0.01, n =  2, mean = -60.7 [0.95], max = -60.6 [0.95]\n",
      "     5: log likelihood = -14.1, policy = 0.00, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "     6: log likelihood = -11.2, policy = 0.01, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      " *   7: log likelihood =  -8.8, policy = 0.27, n = 10, mean = -54.7 [1.00], max = -54.4 [1.00]\n",
      "     8: log likelihood = -10.6, policy = 0.03, n =  0, mean =   0.0 [0.95], max =  -inf [0.00]\n",
      "  g  9: log likelihood =  -8.0, policy = 0.67, n = 20, mean = -60.8 [0.95], max = -57.4 [0.97]\n",
      "Environment step. Action: (4, 1)\n",
      "Computing log likelihood of action (4, 1): ti = 0.0, tj = 46.62428045165643, t_cut = 16.0, lam = 1.5 -> log likelihood = -8.781725883483887\n",
      "Merging particles 4 and 1. New state has 4 particles.\n",
      "4 particles:\n",
      "  p[ 0] = (  1.9,   1.2,   1.1,   1.1)\n",
      "  p[ 1] = (  1.1,   0.4,   0.7,   0.7)\n",
      "  p[ 2] = (  0.6,   0.5,   0.3,   0.2)\n",
      "  p[ 3] = (  0.5,   0.3,   0.3,   0.3)\n",
      "Starting MCTS with 20 trajectories\n",
      "MCTS results:\n",
      "     0: log likelihood = -15.8, policy = 0.00, n =  0, mean =   0.0 [0.97], max =  -inf [0.00]\n",
      "     1: log likelihood = -15.3, policy = 0.00, n =  0, mean =   0.0 [0.97], max =  -inf [0.00]\n",
      "     2: log likelihood = -16.1, policy = 0.00, n =  0, mean =   0.0 [0.97], max =  -inf [0.00]\n",
      "     3: log likelihood = -12.6, policy = 0.37, n = 13, mean = -46.5 [0.99], max = -46.2 [1.00]\n",
      " *g  4: log likelihood = -12.5, policy = 0.55, n = 15, mean = -51.7 [0.95], max = -45.6 [1.00]\n",
      "     5: log likelihood = -14.1, policy = 0.07, n =  2, mean = -48.7 [0.98], max = -48.7 [0.98]\n",
      "Environment step. Action: (3, 1)\n",
      "Computing log likelihood of action (3, 1): ti = 17.626047046255508, tj = 250.2520287784164, t_cut = 16.0, lam = 1.5 -> log likelihood = -12.457528114318848\n",
      "Merging particles 3 and 1. New state has 3 particles.\n",
      "3 particles:\n",
      "  p[ 0] = (  1.9,   1.2,   1.1,   1.1)\n",
      "  p[ 1] = (  1.6,   0.7,   1.0,   1.1)\n",
      "  p[ 2] = (  0.6,   0.5,   0.3,   0.2)\n",
      "Starting MCTS with 5 trajectories\n",
      "MCTS results:\n",
      "     0: log likelihood = -16.2, policy = 0.15, n =  6, mean = -54.5 [0.85], max = -54.5 [0.85]\n",
      " *g  1: log likelihood = -15.3, policy = 0.73, n = 12, mean = -41.9 [0.94], max = -33.2 [1.00]\n",
      "     2: log likelihood = -16.6, policy = 0.12, n =  2, mean = -55.3 [0.84], max = -55.3 [0.84]\n",
      "Environment step. Action: (2, 0)\n",
      "Computing log likelihood of action (2, 0): ti = 42.91672634848851, tj = 108.83427709899843, t_cut = 16.0, lam = 1.5 -> log likelihood = -15.265053749084473\n",
      "Merging particles 2 and 0. New state has 2 particles.\n",
      "2 particles:\n",
      "  p[ 0] = (  2.5,   1.6,   1.3,   1.3)\n",
      "  p[ 1] = (  1.6,   0.7,   1.0,   1.1)\n",
      "Starting MCTS with 1 trajectories\n",
      "MCTS results:\n",
      " *g  0: log likelihood = -17.9, policy = 1.00, n = 13, mean = -40.1 [0.86], max = -17.9 [1.00]\n",
      "Environment step. Action: (1, 0)\n",
      "Computing log likelihood of action (1, 0): ti = 462.6623043913296, tj = 1406.6497382560137, t_cut = 16.0, lam = 3.0 -> log likelihood = -17.922468185424805\n",
      "Merging particles 1 and 0. New state has 1 particles.\n",
      "Episode is done.\n",
      "Sampling new jet with 8 leaves\n",
      "8 particles:\n",
      "  p[ 0] = (  0.8,   0.6,   0.5,   0.3)\n",
      "  p[ 1] = (  0.7,   0.3,   0.4,   0.5)\n",
      "  p[ 2] = (  0.6,   0.4,   0.3,   0.2)\n",
      "  p[ 3] = (  0.6,   0.3,   0.3,   0.4)\n",
      "  p[ 4] = (  0.5,   0.3,   0.3,   0.4)\n",
      "  p[ 5] = (  0.5,   0.2,   0.3,   0.3)\n",
      "  p[ 6] = (  0.3,   0.1,   0.2,   0.2)\n",
      "  p[ 7] = (  0.1,   0.1,   0.1,   0.1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize episode\n",
    "state = env.reset()\n",
    "done = False\n",
    "log_likelihood = 0.\n",
    "errors = 0\n",
    "reward = 0.0\n",
    "agent.set_env(env)\n",
    "agent.eval()\n",
    "\n",
    "# Render initial state\n",
    "env.render()\n",
    "\n",
    "while not done:\n",
    "    # Agent step\n",
    "    action, agent_info = agent.predict(state)\n",
    "    \n",
    "    # Environment step\n",
    "    next_state, next_reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    # Book keeping\n",
    "    log_likelihood += next_reward\n",
    "    errors += int(info[\"legal\"])\n",
    "    agent.update(state, reward, action, done, next_state, next_reward=reward, num_episode=0, **agent_info)\n",
    "    reward, state = next_reward, next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl)",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
